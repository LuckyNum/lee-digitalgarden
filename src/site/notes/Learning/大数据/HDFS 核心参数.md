---
{"dg-publish":true,"permalink":"/Learning/å¤§æ•°æ®/HDFS æ ¸å¿ƒå‚æ•°/"}
---


### 5.1 NameNode å†…å­˜ç”Ÿäº§é…ç½®

1.  NameNode å†…å­˜è®¡ç®—

æ¯ä¸ªæ–‡ä»¶å—å¤§æ¦‚å ç”¨ 150byteï¼Œä¸€å°æœåŠ¡å™¨ 128G å†…å­˜ä¸ºä¾‹ï¼Œèƒ½å­˜å‚¨å¤šå°‘æ–‡ä»¶å—å‘¢?

```
128 * 1024 * 1024 * 1024 / 150Byte â‰ˆ 9.1äº¿
G     MB     KB     Byte
```

2.  Hadoop2. x ç³»åˆ—ï¼Œé…ç½® NameNode å†…å­˜

NameNode å†…å­˜é»˜è®¤ 2000mï¼Œå¦‚æœæœåŠ¡å™¨å†…å­˜ 4Gï¼ŒNameNode å†…å­˜å¯ä»¥é…ç½® 3gã€‚åœ¨hadoop-env.sh æ–‡ä»¶ä¸­é…ç½®å¦‚ä¸‹ã€‚

```
HADOOP_NAMENODE_OPTS=-Xmx3072m
```

3.  Hadoop3. x ç³»åˆ—ï¼Œé…ç½® NameNode å†…å­˜

hadoop-env.sh ä¸­æè¿° Hadoop çš„å†…å­˜æ˜¯åŠ¨æ€åˆ†é…çš„

å…·ä½“ä¿®æ”¹:hadoop-env.sh

```
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS - Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
```

4.  æŸ¥çœ‹ NameNode å ç”¨å†…å­˜

```
[tpxcer@master01 root]$ jps
75153 NameNode
75377 DataNode
75741 NodeManager
76015 JobHistoryServer
45775 Jps
[tpxcer@master01 root]$  jmap -heap 75153
Heap Configuration:
   MaxHeapSize              = 2046820352 (1952.0MB)
[tpxcer@master01 root]$  jmap -heap 75377
Heap Configuration:
   MaxHeapSize              = 2046820352 (1952.0MB)
```

ä¸Šé¢ NameNode å’Œ DataNode å ç”¨å†…å­˜éƒ½æ˜¯è‡ªåŠ¨åˆ†é…çš„ï¼Œä¸”ç›¸ç­‰ã€‚ä¸æ˜¯å¾ˆåˆç†ã€‚ç»éªŒå‚è€ƒ: https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html

### 5.2 NameNode å¿ƒè·³å¹¶å‘é…ç½®

`hdfs-site.xml`

```
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
NameNode æœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹æ± ï¼Œç”¨æ¥å¤„ç†ä¸åŒ DataNode çš„å¹¶å‘å¿ƒè·³ä»¥åŠå®¢æˆ·ç«¯å¹¶å‘ çš„å…ƒæ•°æ®æ“ä½œã€‚
å¯¹äºå¤§é›†ç¾¤æˆ–è€…æœ‰å¤§é‡å®¢æˆ·ç«¯çš„é›†ç¾¤æ¥è¯´ï¼Œé€šå¸¸éœ€è¦å¢å¤§è¯¥å‚æ•°ã€‚é»˜è®¤å€¼æ˜¯ 10ã€‚ <property>
<name>dfs.namenode.handler.count</name>
   <value>21</value>
</property>
```

ä¼ä¸šç»éªŒ: dfs. namenode. handler. count= $20*\log(cluster size)$ï¼Œæ¯”å¦‚é›†ç¾¤è§„æ¨¡ (DataNode å° ğ‘’ æ•°) ä¸º 3 å°æ—¶ï¼Œæ­¤å‚æ•°è®¾ç½®ä¸º 21ã€‚å¯é€šè¿‡ç®€å•çš„ python ä»£ç è®¡ç®—è¯¥å€¼ï¼Œä»£ç å¦‚ä¸‹ã€‚

### 5.3 å¼€å¯å›æ”¶ç«™é…ç½®
å¼€å¯å›æ”¶ç«™åŠŸèƒ½ï¼Œå¯ä»¥å°†åˆ é™¤çš„æ–‡ä»¶åœ¨ä¸è¶…æ—¶çš„æƒ…å†µä¸‹ï¼Œæ¢å¤åŸæ•°æ®ï¼Œèµ·åˆ°é˜²æ­¢è¯¯åˆ é™¤ã€å¤‡ä»½ç­‰ä½œç”¨ã€‚

1.  å¼€å¯å›æ”¶ç«™åŠŸèƒ½å‚æ•°è¯´æ˜

-   é»˜è®¤å€¼ fs.trash.interval = 0ï¼Œ0 è¡¨ç¤ºç¦ç”¨å›æ”¶ç«™;å…¶ä»–å€¼è¡¨ç¤ºè®¾ç½®æ–‡ä»¶çš„å­˜æ´»æ—¶é—´ã€‚
-   é»˜è®¤å€¼ fs.trash.checkpoint.interval = 0ï¼Œæ£€æŸ¥å›æ”¶ç«™çš„é—´éš”æ—¶é—´ã€‚å¦‚æœè¯¥å€¼ä¸º 0ï¼Œåˆ™è¯¥å€¼è®¾ç½®å’Œ fs.trash.interval çš„å‚æ•°å€¼ç›¸ç­‰ã€‚
-   è¦æ±‚ fs.trash.checkpoint.interval <= fs.trash.intervalã€‚

2.  å¯ç”¨å›æ”¶ç«™

ä¿®æ”¹ core-site.xmlï¼Œé…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º 1 åˆ†é’Ÿã€‚
```
<property> 
   <name>fs.trash.interval</name> 
   <value>1</value>
</property>
```

3.  æŸ¥çœ‹å›æ”¶ç«™

```
/user/xxx/.Trash/...
```

4.  é€šè¿‡ç½‘é¡µä¸Šç›´æ¥åˆ é™¤çš„æ–‡ä»¶ä¹Ÿä¸ä¼šèµ°å›æ”¶ç«™ã€‚
5.  é€šè¿‡ç¨‹åºåˆ é™¤çš„æ–‡ä»¶ä¸ä¼šç»è¿‡å›æ”¶ç«™ï¼Œéœ€è¦è°ƒç”¨ moveToTrash()æ‰è¿›å…¥å›æ”¶ç«™

```
Trash trash = New Trash(conf);
trash.moveToTrash(path);
```
1. åªæœ‰åœ¨å‘½ä»¤è¡Œåˆ©ç”¨ hadoop fs -rm å‘½ä»¤åˆ é™¤çš„æ–‡ä»¶æ‰ä¼šèµ°å›æ”¶ç«™ã€‚

### 5.4 NameNode å¤šç›®å½•é…ç½®
åœ¨ hdfs-site. xml æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹

```
<property> 
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
</property>
```

æ£€æŸ¥ name1 å’Œ name2 é‡Œé¢çš„å†…å®¹ï¼Œå‘ç°ä¸€æ¨¡ä¸€æ ·ã€‚

### 5.4 DataNode å¤šç›®å½•é…ç½®
```
<property> 
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp. dir}/dfs/data2</value>
</property>
```

### 5.5 é›†ç¾¤æ•°æ®å‡è¡¡ä¹‹ç£ç›˜é—´æ•°æ®å‡è¡¡
ç”Ÿäº§ç¯å¢ƒï¼Œç”±äºç¡¬ç›˜ç©ºé—´ä¸è¶³ï¼Œå¾€å¾€éœ€è¦å¢åŠ ä¸€å—ç¡¬ç›˜ã€‚åˆšåŠ è½½çš„ç¡¬ç›˜æ²¡æœ‰æ•°æ®æ—¶ï¼Œå¯ä»¥æ‰§è¡Œç£ç›˜æ•°æ®å‡è¡¡å‘½ä»¤ã€‚(Hadoop3. x æ–°ç‰¹æ€§)

1.  ç”Ÿæˆå‡è¡¡è®¡åˆ’(æˆ‘ä»¬åªæœ‰ä¸€å—ç£ç›˜ï¼Œä¸ä¼šç”Ÿæˆè®¡åˆ’)

```
hdfs diskbalancer -plan hadoop103
```

2.  æ‰§è¡Œå‡è¡¡è®¡åˆ’

```
hdfs diskbalancer -execute hadoop103.plan.json
```

3.  æŸ¥çœ‹å½“å‰å‡è¡¡ä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µ

```
hdfs diskbalancer -query hadoop103
```

4.  å–æ¶ˆå‡è¡¡ä»»åŠ¡

```
hdfs diskbalancer -cancel hadoop103.plan.json
```



