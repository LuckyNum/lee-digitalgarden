---
{"dg-publish":true,"permalink":"/Learning/机器学习/K-近邻算法/"}
---


## 1 什么是K-近邻算法
![Pasted image 20221218131534.png|800](/img/user/Attachments/Pasted%20image%2020221218131534.png)
> 根据你的“邻居”来推断出你的类别

### 1.1 K-近邻算法(KNN)概念
K Nearest Neighbor算法又叫KNN算法，这个算法是[[Learning/机器学习/机器学习.excalidraw\|机器学习.excalidraw]]里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法。
#### 定义
如果一个样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别**，则该样本也属于这个类别。
> 来源：KNN算法最早是由[[Cover\|Cover]]和[[Hart\|Hart]]提出的一种[[Inbox/分类算法\|分类算法]]
#### 距离公式
两个样本的距离可以通过如下公式计算，又叫[[Inbox/欧式距离\|欧式距离]] ，关于[[Inbox/距离度量\|距离度量]]会在后面进行讨论
![欧式距离.png|800](/img/user/Attachments/%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB.png)
- 二维平面上点 $a(x1, y1)$ 与 $b(x2, y2)$ 间的[[Inbox/欧式距离\|欧式距离]]
  $d12 = \sqrt{(x1-x2)^2+(y1-y2)^2}$
  
- 三维平面上点 $a(x1, y1, z1)$ 与 $b(x2, y2, z2)$ 间的[[Inbox/欧式距离\|欧式距离]]
  $d12 = \sqrt{(x1-x2)^2+(y1-y2)^2+(z1-z2)^2}$
  
- n维空间点 $a(x11, x12, ..., x1n)$ 与 $b(x21, x22, ..., x2n)$ 间的[[Inbox/欧式距离\|欧式距离]]（两个n维向量）
  $d12 = \sqrt{\sum^{n}_{k=1}{(x1k-x2k)^2}}$

### 1.2 电影类型分析
假设我们现在有几部电影
![Pasted image 20221218133235.png](/img/user/Attachments/Pasted%20image%2020221218133235.png)
其中 9 号电影不知道类别，如何去预测？我们可以利用K近邻算法的思想
![knn电影举例2.png](/img/user/Attachments/knn%E7%94%B5%E5%BD%B1%E4%B8%BE%E4%BE%8B2.png)

分别计算每个电影和被预测电影的距离，然后求解
![Pasted image 20221218133334.png](/img/user/Attachments/Pasted%20image%2020221218133334.png)
### 1.3 KNN算法流程总结
1）计算已知类别数据集中的点与当前点之间的距离
2）按距离递增次序排序
3）选取与当前点距离最小的k个点
4）统计前k个点所在的类别出现的频率
5）返回前k个点出现频率最高的类别作为当前点的预测分类

- 定义:就是通过你的"邻居"来判断你属于哪个类别
- 如何计算你到你的"邻居"的距离：一般时候,都是使用欧氏距离

## 2 K-近邻算法API
```python
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)
```
- 参数介绍：
    - n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数

## 3 案例
### 3.1 步骤分析
- 1.获取数据集
- 2.数据基本处理（该案例中省略）
- 3.特征工程（该案例中省略）
- 4.机器学习
- 5.模型评估（该案例中省略）

### 3.2 代码过程
-   步骤一：导入模块
```python
from sklearn.neighbors import KNeighborsClassifier
```

-   步骤二：构造数据集
-   数据集格式一：
```python
x = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
```

-   数据集格式二：
```python
x = [[39,0,31],[3,2,65],[2,3,55],[9,38,2],[8,34,17],[5,2,57],[21,17,5],[45,2,9]]
y = [0,1,2,2,2,2,1,1]
```

-   步骤三：机器学习 -- 模型训练
```python
# 实例化API
estimator = KNeighborsClassifier(n_neighbors=1)
# 使用fit方法进行训练
estimator.fit(x, y)

estimator.predict([[1]])

# 数据集格式二对应的测试数据
# estimator.predict([[23,3,17]])
```

## 4 距离度量
[[Inbox/距离度量\|距离度量]]

## 5 K 值选择
### K 值选择说明
-   **举例说明：**
![Pasted image 20221220184219.png](/img/user/Attachments/Pasted%20image%2020221220184219.png)
-   **K值过小**：
    -   容易受到异常点的影响
-   **k值过大：**
    -   受到样本均衡的问题

### K 值选择问题
**李航博士的一书[[《统计学习方法》\|《统计学习方法》]]上所说：**
-   1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，
    -   “学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，
    -   换句话说，**K值的减小就意味着整体模型变得复杂，容易发生过拟合；**
-   2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，
    -   其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，**与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误。**
    -   **且K值的增大就意味着整体的模型变得简单。**
-   3) K=N（N为训练样本个数），则完全不足取，
    -   因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。
-   在**实际应用中，K 值一般取一个比较小的数值**，例如采用交叉验证法（简单来说，就是把训练数据在分成两组: 训练集和验证集）来选择最优的 K 值。
### 误差
-   **近似误差**：
    -   对现有训练集的训练误差，**关注训练集**，
    -   如果**近似误差过小可能会出现过拟合的现象**，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。
    -   模型本身不是最接近最佳模型。
-   **估计误差**：
    -   可以理解为对测试集的测试误差，**关注测试集**，
    -   估计误差小说明对未知数据的预测能力好，
    -   模型本身最接近最佳模型。